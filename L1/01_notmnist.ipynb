{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load packages\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display, Image\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import ndimage\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sys\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define environment variables\n",
    "\n",
    "HOMEDIR = os.environ[\"HOME\"]\n",
    "\n",
    "BASEDIR = os.path.join(HOMEDIR, \"workspace\", \"self_education\", \"udacity\", \"TensorFlowUdacity\")\n",
    "\n",
    "DATADIR = os.path.join(BASEDIR, \"data\")\n",
    "ORIGINALDATADIR = os.path.join(DATADIR, \"notmnist\", \"original\")\n",
    "SANITIZEDDATADIR = os.path.join(DATADIR, \"notmnist\", \"sanitized\")\n",
    "\n",
    "NOTMNISTLARGEDIR = os.path.join(ORIGINALDATADIR, \"notmnist_large\")\n",
    "NOTMNISTSMALLDIR = os.path.join(ORIGINALDATADIR, \"notmnist_small\")\n",
    "\n",
    "ORIGINALDATAFILE = os.path.join(ORIGINALDATADIR, 'notmnist.pickle')\n",
    "SANITIZEDDATAFILE = os.path.join(SANITIZEDDATADIR, 'notmnist.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Functions for getting array of directory paths and array of file paths\n",
    "\n",
    "def get_dir_paths(root):\n",
    "  return [os.path.join(root, n) \n",
    "    for n in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, n))\n",
    "  ]\n",
    "\n",
    "def get_file_paths(root):\n",
    "  return [os.path.join(root, n)\n",
    "    for n in sorted(os.listdir(root))\n",
    "    if os.path.isfile(os.path.join(root, n))\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Get directory and file paths of training and test sets\n",
    "\n",
    "train_data_paths = get_dir_paths(NOTMNISTLARGEDIR)\n",
    "test_data_paths = get_dir_paths(NOTMNISTSMALLDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACU0lEQVR4nFWRa2iOYRjHf9d93887\n79Zr22tYbLx2MnM+pBYlJZ+mhPhqFEs0fPVBSVgkh7RITJKQkcO2NqeaSFoboqRIjfgwswM7vM/z\nXD48o3z+9b/6/3+X3ChKpr5/jilSPq7/g1GSKZ568TCjxNsLTNj8IhvgjTYBJCsvNSQg0au7HfQ0\nFDzqyAzNVCwm5Mfz57taj5lCD3HgtK2yzgXe0lwAxMjp7saLAxYMBEHPSG5Obo5FADTwvVtbbe0A\n6gAGZ+wAkxNBML53Yfq1wYkAwoyzAK+0DRNhHI2qexzA7yXnBFuAQUE0NfpVZcviwhAQpt2smFOx\n8LM+RMDK+21YS5XWRnc08INAAYgF68p+QGDvPch1ACNldQEmB2NFRqcfVwOEsikOCEX1AF3aBiy9\nfko3YsdqAb+WNYZIiuSKScvXVpcQRjYiGPt4WNQtyk6dKsn8tH7lWE6jYeL39f3s83k5P39Dd8sh\ngn9rgZHSo2AmAgM3n+zLG1MhBhCKT0aGmsV6xF8EG/4rVHketQWIatoNbe/MAISsxQ7A+3LVaGw1\nAL7pahgCrH+wygGYwfcmzEijAKFUQ2D9eTufOYDheWdEbXLsZSqKaNZlGziArPadwOu5kV0Usf6J\nUqwDGFpwwKid/PfZ4NLH+u5ujJJezzvUjv5F4tI1hZvugAGx2d/utNxvHkQBMU7TtfVf9s/GOCAo\nXXUlQ80UBFANE0fy1ow3wygQW9FeLlZib7UJJDGzpnUbQKs+ltuFsTnDnTHUzIr3fxCbmZ/o7cgT\ntDjR/gcvk9pKvg1kBgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABcklEQVR4nHWSMU9UURCFv7kz75EF\nOokgSwxbmKidiZHQSWIsLeik4RdQkdDYQElBxU/wB9jQWNtotFQDNKwhoXETJQFB2HvHYt973GfC\n6U7OnHtOZq68+Z4AkHT/9G13/QM1ffRtz768p8LUyr3Jr+9qxsHzudBRU1VVU7b2Z0VLVVXVQmX3\n0FIUBxCPZUdSJALgKRaFcYM4JIdjwWqniSCmAkBIJth5M/7TkcthQwdgS/Pi4iA+UfL3ScdS5Zx2\neLzqNYZX3ek1zxB+/W4aaPGiqzeFLs5ks1/6qBIQvepDiL3Pe7zkFvQ2FmzcGh/glZGQxraxNMzF\nBiFFyDekkPJBb63vP7dkoj1TjvutkFeMOgh3rtx36pzAA/eQRZxH2nfJnnXV5U8hDw6t0Y8n+K3i\npLRoW0ztfeQ/ARCtpkMysD+tn+DXjW0AtjgzVrHxksuHCzq6qPhdoPc6O/381NN+Ru0oXFeHEqMY\nDH7Mxspr/AMY9qSsSA/q8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAByElEQVR4nK2SPWhTYRSGn/Pdiza9\nNQVtKwEHJSIWW52UCi0I/oCCSuvi4iKIqNjNSXAUROjgIDgIggShddShm1pTf0BRrBJQCmra+puK\n9xZN7v2+49DExmg23/E873l5ORyhZ+eWTDqJoij66QdB0Bp44Wzh8fMSwIWS078VF3MD4J9OxUYw\nFV8VAQQweEF7AuwP1Vo9lj3ywtpqxsfcqe0rAaD/QUXt+QyXQ3U2carFswBiAEPq+NOyTgxmDox+\nVlWX3N1qfE8WNw2sOJj7qk+GuzccvvrG6u2OKkFAjIXs0FCvN3Vr4kO6N7ohIuqq5RDjFOnq27t7\nbfjy3mRh2oKptgdE5GL2/sNiavOOgfW8zY/n31Ovm6p2ZuzoOukYvDaXvBtZVQ/HktiqavjoBKbf\nankczG8oni/O2WDbLo9yJfY21kNAjPFw1rqFKb90DnwaJGLg0yX77NU/IABfroNxTSBG1DXbxNUu\n21z/AcrSTBqg/JFgpA4aH7fkF3F4fs3tAZ1npmtvOTvS0wIYs+jvPLSnr8ugKAgiPwqTd/JzAKRO\nzjjVhtd230f3+T7Er69s6l7TsjA//y3SZan2dFvb8jgMW1frL9du4mGHt7GzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABM0lEQVR4nNWQsUoDQRRF75tZIyRC\nCkWipJb4B2IhWFjYWAmWFvoBItgqCBZ+gJX6D5amsxOsbBQFIWAsDEYJRhKzO/OuReLuJsH03uq9\nOZzLYwQCnm21w+ZHM1JVpRERzeYnctM3AggXFvOl+dIYkrhK5bl2m+yzRy0655xzvnMwY+J3EwRW\nLPaoJBnxFDDWBtYAgDrnIbhoCAEoHjCu3jsf6/Soh725A+02xt3El0d/TGpWjoCDYh/ECFNGweH8\nD6h/Qlp8DkOa7mK/X4e+Q1CsU0nH6wxkwBQUMgAoOA/tryk2sAAQYIdKH/Gk58WqhViUGUbK4/SN\nxbXNlSxMgNXQKV82YBO236Tn47pgqUq+HU7BSAIrpCr16Z6ty+1JSLpU5naXC7n2e+OufFUlLDUF\nfwCAgIu8B8+knwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABwUlEQVR4nHWSv2uTURSGn3PPbUwa\nBbEUUwNmKIhCYhW0FCcRg8GiBQddnKUdBHFx0b9BHFxcRUEHQRQEsRlEkNAMQYVYf9BAhwZUkFry\nNTX5jsNXmy8lfad7ODz3vOe9V4ikoZE9W8xnR4Sw3axXXn1cRwAQCYcuXp9a+1D73Gyl9uUmJsZZ\nfvAEACdcatjrC2n+Sw7drNs9AGX0mc3nEZxXVVXvBRIz+wGlsGSziFPZIlEvCDgKP4Ii3tEvUUHI\nLNo0Q8IACY/tNn5QCyjawi4GcsALu7wjeKzbSO0E+pJ7Hmg3ZmFLhp+i0gdanCxQJ4zVyagQlXZA\nKxiLky+tY2Zm4S+7g0+tBb2BlkmXV7tmhMMlFuF3OxsjN0+Ok9Y6gFtJZGJNE6fe+wRnqDVxFU4Q\ny9zCbqfT2aDA+9C7N5Torbl5eZg8RRXY+9360Gjk0XA9h4Mb9hTdFhvXbEERSNdsdttzKvftLgrK\n5J+NaXwMdi5Rs6t4QDm32plDJPpc6kXIBe3xyIhy/JO9O90zNTJTtqoiCODC4blbo4356nI7vefg\nkfzh5JeHj5bEonDUwt3nr0yOKXR+rnwrv/36F4B/ZBGXW8xs5MsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Display a sample of 5 images in their initial png format\n",
    "\n",
    "nsamples = 5\n",
    "\n",
    "for i in np.arange(nsamples):\n",
    "  display(Image(filename=np.random.choice(get_file_paths(np.random.choice(test_data_paths)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set image properties\n",
    "\n",
    "image_size = 28 # Pixel width and height\n",
    "pixel_depth = 255.0  # Number of levels per pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read a sample image\n",
    "\n",
    "image_file = np.random.choice(get_file_paths(np.random.choice(test_data_paths)))\n",
    "image_data = ndimage.imread(image_file).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Show type of image object\n",
    "\n",
    "type(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Show dimensions of image object\n",
    "\n",
    "image_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11106de90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot image using imshow\n",
    "\n",
    "plt.imshow(image_data)\n",
    "plt.show()\n",
    "# display(Image(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11106d750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot image using a scatterplot\n",
    "\n",
    "colors = [str(i/pixel_depth) for i in np.ravel(image_data)]\n",
    "plt.scatter(\n",
    "  np.tile(np.arange(image_size), image_size),\n",
    "  np.repeat(np.flipud(np.arange(image_size)), image_size),\n",
    "  s=150,\n",
    "  c=colors,\n",
    "  marker='s'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11106db50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot image using a scatterplot by setting cmap option\n",
    "\n",
    "colors = [str(i/pixel_depth) for i in np.ravel(image_data)]\n",
    "plt.scatter(\n",
    "  np.tile(np.arange(image_size), image_size),\n",
    "  np.repeat(np.flipud(np.arange(image_size)), image_size),\n",
    "  s=150,\n",
    "  c=colors,\n",
    "  marker='s',\n",
    "  cmap=plt.cm.winter\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Functions for normalizing and retrieving images\n",
    "\n",
    "# Normalize image by pixel depth\n",
    "def normalize_image(image, pixel_depth):\n",
    "  return (ndimage.imread(image).astype(float)-pixel_depth/2)/pixel_depth\n",
    "\n",
    "# Retrieve original image from normalized image\n",
    "def unnormalize_image(image, pixel_depth):\n",
    "  return (pixel_depth*image+pixel_depth/2).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function for loading data for a single letter\n",
    "\n",
    "def load_letter(root, image_size, pixel_depth, verbose=True, min_nimages=1):\n",
    "  \"\"\"Load data for a single letter.\"\"\"\n",
    "\n",
    "  if verbose:\n",
    "    print(root)\n",
    "\n",
    "  image_files = get_file_paths(root)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32)\n",
    "\n",
    "  image_index = 0\n",
    "  for image in image_files:\n",
    "    try:\n",
    "      image_data = normalize_image(image, pixel_depth)\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[image_index, :, :] = image_data\n",
    "      image_index += 1\n",
    "    except IOError as e:\n",
    "      print('Skipping because of not being able to read: ', image_file)\n",
    "\n",
    "  dataset = dataset[0:image_index, :, :]\n",
    "  if image_index < min_nimages:\n",
    "    raise Exception('Fewer images than expected: %d < %d' % (image_index, min_nimages))\n",
    "\n",
    "  if verbose:    \n",
    "    print('Full dataset tensor: ', dataset.shape)\n",
    "    print('Mean: ', np.mean(dataset))\n",
    "    print('Standard deviation: ', np.std(dataset))\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_small/C\n",
      "('Full dataset tensor: ', (1873, 28, 28))\n",
      "('Mean: ', -0.14152053)\n",
      "('Standard deviation: ', 0.44269028)\n"
     ]
    }
   ],
   "source": [
    "## Test load_letter() function by loading data for letter C\n",
    "\n",
    "letter_data = load_letter(test_data_paths[2], image_size, pixel_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1873, 28, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Show dimensions of loaded data for letter C\n",
    "\n",
    "letter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Show dimensions of data for the first image of letter C\n",
    "\n",
    "letter_data[0, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function for generating pickle filenames\n",
    "\n",
    "def generate_pickle_names(root):\n",
    "  dataset_files = []\n",
    "\n",
    "  for d in root:\n",
    "    pickle_file = d + '.pickle'\n",
    "    dataset_files.append(pickle_file)\n",
    "  \n",
    "  return dataset_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function for pickling data of all letters\n",
    "\n",
    "def pickle_letters(root, image_size, pixel_depth, verbose=True, min_nimages=1, force=False):\n",
    "  pickle_files = generate_pickle_names(root)\n",
    "  n = len(root)\n",
    "    \n",
    "  for i in np.arange(n):\n",
    "    if os.path.exists(pickle_files[i]) and not force:\n",
    "      print('%s already present, skipping pickling' % pickle_files[i])\n",
    "    else:\n",
    "      print('Pickling %s' % pickle_files[i])\n",
    "      dataset = \\\n",
    "        load_letter(root[i], image_size, pixel_depth, verbose=verbose, min_nimages=min_nimages)\n",
    "      try:\n",
    "        with open(pickle_files[i], 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', pickle_files[i], ':', e)\n",
    "  \n",
    "  return pickle_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_large/A.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_large/B.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_large/C.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_large/D.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_large/E.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_large/F.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_large/G.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_large/H.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_large/I.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_large/J.pickle already present, skipping pickling\n"
     ]
    }
   ],
   "source": [
    "### Pickle training set\n",
    "\n",
    "train_files = pickle_letters(train_data_paths, image_size, pixel_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_small/A.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_small/B.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_small/C.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_small/D.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_small/E.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_small/F.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_small/G.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_small/H.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_small/I.pickle already present, skipping pickling\n",
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist_small/J.pickle already present, skipping pickling\n"
     ]
    }
   ],
   "source": [
    "### Pickle test set\n",
    "\n",
    "test_files = pickle_letters(test_data_paths, image_size, pixel_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test pickling by loading pickle for letter D\n",
    "\n",
    "letter_data = np.load(os.path.join(NOTMNISTLARGEDIR, \"D.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52911, 28, 28)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Show dimensions of data loaded from pickle of letter D\n",
    "\n",
    "letter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niZxi+ImcYviJnGL4iZxi+ImcYviJnPp/wazOq4YLE/oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114b6c550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot fifth image of letter D from loaded pickle using imshow\n",
    "\n",
    "plt.imshow(letter_data[4, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52909, 52911, 52912, 52911, 52912, 52912, 52912, 52912, 52912, 52911])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check if training sets are balanced across letter classes using absolute frequencies\n",
    "\n",
    "ntrainsets = len(train_data_paths)\n",
    "\n",
    "train_stats = np.empty(shape=ntrainsets, dtype=np.int64)\n",
    "\n",
    "for i in np.arange(ntrainsets):\n",
    "  letter_data = np.load(\".\".join([train_data_paths[i], \"pickle\"]))\n",
    "  train_stats[i] = letter_data.shape[0]\n",
    "\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.99954641,   9.9999244 ,  10.0001134 ,   9.9999244 ,\n",
       "        10.0001134 ,  10.0001134 ,  10.0001134 ,  10.0001134 ,\n",
       "        10.0001134 ,   9.9999244 ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check if training sets are balanced across letter classes using percentages\n",
    "\n",
    "train_stats_perc = 100*train_stats/np.float32(sum(train_stats))\n",
    "\n",
    "train_stats_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1872, 1873, 1873, 1873, 1873, 1872, 1872, 1872, 1872, 1872])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check if test sets are balanced across letter classes using absolute frequencies\n",
    "\n",
    "ntestsets = len(test_data_paths)\n",
    "\n",
    "test_stats = np.empty(shape=ntestsets, dtype=np.int64)\n",
    "\n",
    "for i in np.arange(ntrainsets):\n",
    "  letter_data = np.load(\".\".join([test_data_paths[i], \"pickle\"]))\n",
    "  test_stats[i] = letter_data.shape[0]\n",
    "\n",
    "test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.9978637 ,  10.00320444,  10.00320444,  10.00320444,\n",
       "        10.00320444,   9.9978637 ,   9.9978637 ,   9.9978637 ,\n",
       "         9.9978637 ,   9.9978637 ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check if test sets are balanced across letter classes using percentages\n",
    "\n",
    "test_stats_perc = 100*test_stats/np.float32(sum(test_stats))\n",
    "\n",
    "test_stats_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function for allocating array to host all image sub-arrays into it\n",
    "\n",
    "def allocate_image_space(nimages, image_size):\n",
    "  if nimages > 0:\n",
    "    dataset = np.ndarray((nimages, image_size, image_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nimages, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function for merging letter images to create train, validation and test sets\n",
    "\n",
    "def merge_images(pickle_files, image_size, ntrain, nvalidation=0):\n",
    "  train_dataset, train_labels = allocate_image_space(ntrain, image_size)\n",
    "  validation_dataset, validation_labels = allocate_image_space(nvalidation, image_size)\n",
    "\n",
    "  nclasses = len(pickle_files)\n",
    "  ntrain_per_class = ntrain//nclasses\n",
    "  nvalidation_per_class = nvalidation//nclasses\n",
    "  total_per_class = ntrain_per_class+nvalidation_per_class\n",
    "  train_start, validation_start = 0, 0\n",
    "  train_end, validation_end = ntrain_per_class, nvalidation_per_class\n",
    "\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_data = pickle.load(f)\n",
    "        \n",
    "        # Shuffle the letters to have random training and validation set\n",
    "        np.random.shuffle(letter_data)\n",
    "\n",
    "        train_dataset[train_start:train_end, :, :] = letter_data[:ntrain_per_class, :, :]\n",
    "        train_labels[train_start:train_end] = label\n",
    "        train_start += ntrain_per_class\n",
    "        train_end += ntrain_per_class\n",
    "        \n",
    "        if validation_dataset is not None:\n",
    "          validation_dataset[validation_start:validation_end, :, :] = \\\n",
    "            letter_data[ntrain_per_class:total_per_class, :, :]\n",
    "          validation_labels[validation_start:validation_end] = label\n",
    "          validation_start += nvalidation_per_class\n",
    "          validation_end += nvalidation_per_class\n",
    "\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "        \n",
    "  return train_dataset, train_labels, validation_dataset, validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set number of train, validation and test images\n",
    "\n",
    "ntrain = 200000\n",
    "nvalidation = 10000\n",
    "ntest = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Generate train and validation datasets\n",
    "\n",
    "train_dataset, train_labels, validation_dataset, validation_labels = \\\n",
    "  merge_images(train_files, 28, ntrain, nvalidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Generate test dataset\n",
    "\n",
    "test_dataset, test_labels, _, _ = merge_images(test_files, 28, ntest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training: ', (200000, 28, 28), (200000,))\n",
      "('Validation: ', (10000, 28, 28), (10000,))\n",
      "('Testing: ', (10000, 28, 28), (10000,))\n"
     ]
    }
   ],
   "source": [
    "## Show dimensions of train, validation and test datasets\n",
    "\n",
    "print('Training: ', train_dataset.shape, train_labels.shape)\n",
    "print('Validation: ', validation_dataset.shape, validation_labels.shape)\n",
    "print('Testing: ', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function for shuffling image datasets\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation, :, :]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Shuffle train, validation and test datasets\n",
    "\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "validation_dataset, validation_labels = randomize(validation_dataset, validation_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACFCAYAAABL2gNbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEISAIB26IAhCQPh/20PS0NBwJs8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114b8b2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Display a sample of 3 images in their initial png format\n",
    "\n",
    "samples = np.random.choice(3, 3, replace=False)\n",
    "\n",
    "fig = plt.figure()\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "fig.add_subplot(gs[0])\n",
    "plt.imshow(train_dataset[samples[0], :, :])\n",
    "fig.add_subplot(gs[1])\n",
    "plt.imshow(train_dataset[samples[1], :, :])\n",
    "fig.add_subplot(gs[2])\n",
    "plt.imshow(train_dataset[samples[2], :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32),\n",
       " array([20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Count number of letters per class in shuffled train dataset using labels\n",
    "\n",
    "np.unique(train_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32),\n",
       " array([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Count number of letters per class in shuffled validation dataset using labels\n",
    "\n",
    "np.unique(validation_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32),\n",
       " array([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Count number of letters per class in shuffled test dataset using labels\n",
    "\n",
    "np.unique(test_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function for saving an object to a pickle file\n",
    "\n",
    "def save_to_pickle(pickle_file, object, force=False):\n",
    "  if os.path.exists(pickle_file) and not force:\n",
    "    print('%s already present, skipping pickling' % pickle_file)\n",
    "  else:\n",
    "    try:\n",
    "      f = open(pickle_file, 'wb')\n",
    "      pickle.dump(object, f, pickle.HIGHEST_PROTOCOL)\n",
    "      f.close()\n",
    "    except Exception as e:\n",
    "      print('Unable to save object to', pickle_file, ':', e)\n",
    "      raise      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/original/notmnist.pickle already present, skipping pickling\n"
     ]
    }
   ],
   "source": [
    "## Save train, validation and test datasets to pickle file\n",
    "\n",
    "save_to_pickle(\n",
    "  ORIGINALDATAFILE,\n",
    "  {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'validation_dataset': validation_dataset,\n",
    "    'validation_labels': validation_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "  }    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 690800451\n"
     ]
    }
   ],
   "source": [
    "## Size of picle file containing train, validation and test datasets\n",
    "\n",
    "print('Compressed pickle size: %d' % os.stat(ORIGINALDATAFILE).st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether two images are duplicates it suffices to check if their matrix representations are equal. The main limitation of this approach is that it only detecs exact duplicates. There are other methods for identifying near duplicates.\n",
    "\n",
    "There are two main classes of methods for detecting near duplicates among images. One of these two method classes is known as image fingerprinting or image hashing. The main idea of image hashing is to construct a unique numerical value, known as image hash, from the contents of the image. The hash plays the role of a fingerprint in the sense that it uniquely identifies its associated image. Images that are “similar” should have “similar” hashes.\n",
    "\n",
    "Another class of algorithms compares a pair of images by computing a metric of similarity based on the contents of both images. Various similarity metrics are used in practice, such as the mean squared error (MSE) or the structural similarity index (SSIM).\n",
    "\n",
    "In this notebook, near-duplicates are found using image hashing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Finding near-duplicates using image hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Difference hashing (dHash) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential approach to image hash creation is to make use of cryptographic hashing algorithms such as MD5 or SHA-1. However, if an image has been edited even slightly, the [avalance effect](https://en.wikipedia.org/wiki/Avalanche_effect) of the cryptographic algorithm would generate a hash for the slightly edited image very different from the hash of the original image.\n",
    "\n",
    "For this reason, alternative hashing algorithms are used for image hashing. In what follows, the difference hashing algorithm (dHash) will be presented. dHash computes the difference in brightness between adjacent pixels. It is a simple algorithm to implement. Its main steps are the following:\n",
    "1. Grayscale the image.\n",
    "2. Shrink the image.\n",
    "3. Compare the intensity values of adjacent pixels row-wise. Introduce a binary classification scheme to compare adjacent pixels; if the intensity value of the preceding pixel is higher than the intensity value of the succeeding pixel in the row, then label the difference between the two adjacent pixels as 1, otherwise as 0.\n",
    "4. Convert the resulting binary vector of ones and zeros to a hexadecimal string, which is the image hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function for computing the difference hash (dHash) of an image\n",
    "\n",
    "# The input argument hsize is the hash size\n",
    "def image_dhash(image, hsize=8):\n",
    "  # Grayscale and shrink the image\n",
    "  icon = PIL.Image.fromarray(image).convert('L').resize((hsize+1, hsize), PIL.Image.ANTIALIAS)\n",
    "  icon = np.array(icon)\n",
    "\n",
    "  # Compare intensity values of adjacent pixels row-wise\n",
    "  diff = np.empty([hsize, hsize], dtype=np.bool_)\n",
    "  for row in np.arange(hsize):\n",
    "    for col in np.arange(hsize):\n",
    "      diff[row, col] = icon[row, col] > icon[row, col+1]\n",
    "\n",
    "  # Convert binary vector to hexadecimal string\n",
    "  hexadecimal = np.empty(hsize, dtype=np.dtype((bytes, hsize/4)))\n",
    "  for i in np.arange(hsize):\n",
    "    hexadecimal[i] = \\\n",
    "      hex(int(''.join(str(b) for b in np.flipud(diff[i, :].astype(int))), 2))[2:].rjust(2, '0')\n",
    "    \n",
    "  return ''.join(hexadecimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set hash size, which will be used for generate difference hashes of images\n",
    "\n",
    "hash_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compute difference hashes of images in the train dataset\n",
    "\n",
    "train_dhashes = np.empty(ntrain, dtype=np.dtype((bytes, (hash_size**2)/4)))\n",
    "\n",
    "for i in np.arange(ntrain):\n",
    "  train_dhashes[i] = image_dhash(unnormalize_image(train_dataset[i, :, :], pixel_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Compute difference hashes of images in the validation dataset\n",
    "\n",
    "validation_dhashes = np.empty(nvalidation, dtype=np.dtype((bytes, (hash_size**2)/4)))\n",
    "\n",
    "for i in np.arange(nvalidation):\n",
    "  validation_dhashes[i] = \\\n",
    "    image_dhash(unnormalize_image(validation_dataset[i, :, :], pixel_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compute difference hashes of images in the test dataset\n",
    "\n",
    "test_dhashes = np.empty(ntest, dtype=np.dtype((bytes, (hash_size**2)/4)))\n",
    "\n",
    "for i in np.arange(ntest):\n",
    "  test_dhashes[i] = image_dhash(unnormalize_image(test_dataset[i, :, :], pixel_depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing near-duplicates within each of train, validation and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in train dataset: 200000\n",
      "Number of images in train dataset after excluding near-duplicates: 156007\n",
      "78.00% of images in train dataset kept\n"
     ]
    }
   ],
   "source": [
    "## Get locations of images in the train dataset after excluding near-duplicates\n",
    "\n",
    "unique_train_dhashes, unique_train_locations = np.unique(train_dhashes, return_index=True)\n",
    "\n",
    "print('Number of images in train dataset: %d' % ntrain)\n",
    "print(\n",
    "  'Number of images in train dataset after excluding near-duplicates: %d'\n",
    "  % np.size(unique_train_locations)\n",
    ")\n",
    "print(\n",
    "  '%.2f%% of images in train dataset kept' \n",
    "  % round(100*np.float32(np.size(unique_train_locations))/ntrain, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in validation dataset: 10000\n",
      "Number of images in validation dataset after excluding near-duplicates: 9281\n",
      "92.81% of images in validation dataset kept\n"
     ]
    }
   ],
   "source": [
    "## Get locations of images in the validation dataset after excluding near-duplicates\n",
    "\n",
    "unique_validation_dhashes, unique_validation_locations = \\\n",
    "  np.unique(validation_dhashes, return_index=True)\n",
    "\n",
    "print('Number of images in validation dataset: %d' % nvalidation)\n",
    "print(\n",
    "  'Number of images in validation dataset after excluding near-duplicates: %d'\n",
    "  % np.size(unique_validation_locations)\n",
    ")\n",
    "print(\n",
    "  '%.2f%% of images in validation dataset kept' \n",
    "  % round(100*np.float32(np.size(unique_validation_locations))/nvalidation, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in test dataset: 10000\n",
      "Number of images in test dataset after excluding near-duplicates: 8983\n",
      "89.83% of images in test dataset kept\n"
     ]
    }
   ],
   "source": [
    "## Get locations of images in the test dataset after excluding near-duplicates\n",
    "\n",
    "unique_test_dhashes, unique_test_locations = np.unique(test_dhashes, return_index=True)\n",
    "\n",
    "print('Total number of images in test dataset: %d' % ntest)\n",
    "print(\n",
    "  'Number of images in test dataset after excluding near-duplicates: %d'\n",
    "  % np.size(unique_test_locations)\n",
    ")\n",
    "print(\n",
    "  '%.2f%% of images in test dataset kept' \n",
    "  % round(100*np.float32(np.size(unique_test_locations))/ntest, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing near-duplicates between train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function for returning locations (array indices) of unique images between datasets\n",
    "## Uniqueness of images is defined on the basis of uniqueness of dHashes\n",
    "## The returned indices are a subset of scannedlocs\n",
    "## scannedlocs is a subset of baselocs\n",
    "\n",
    "def unique_image_crosslocations(scannedset, scannedlocs, baseset, baselocs):\n",
    "  indices = np.array([], dtype=np.int64)\n",
    "\n",
    "  for i in scannedlocs:\n",
    "    if scannedset[i] not in baseset[baselocs]:\n",
    "      indices = np.append(indices, i)\n",
    "\n",
    "  return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images within train dataset: 156007\n",
      "Number of unique images in train dataset not in test set: 153261\n",
      "98.24% of unique images in train dataset kept\n"
     ]
    }
   ],
   "source": [
    "## Get locations of images whose dHashes appear in train but not in test set\n",
    "\n",
    "unique_train_locations_vs_test = unique_image_crosslocations(\n",
    "  train_dhashes, unique_train_locations, test_dhashes, unique_test_locations\n",
    ")\n",
    "\n",
    "print(\n",
    "  'Number of unique images within train dataset: %d'\n",
    "  % np.size(unique_train_locations)\n",
    ")\n",
    "print(\n",
    "  'Number of unique images in train dataset not in test set: %d'\n",
    "  % np.size(unique_train_locations_vs_test)\n",
    ")\n",
    "print(\n",
    "  '%.2f%% of unique images in train dataset kept' \n",
    "  % round(\n",
    "    100*np.float32(np.size(unique_train_locations_vs_test))/\n",
    "      np.size(unique_train_locations),\n",
    "    2\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images in train set and not in test set: 153261\n",
      "Number of unique images in train set and not in test or validation set: 151262\n",
      "98.70% of unique images in train dataset kept\n",
      "75.63% of images from original train dataset kept\n"
     ]
    }
   ],
   "source": [
    "## Get locations of images whose dHashes appear in train only\n",
    "\n",
    "unique_train_locations_vs_validation = unique_image_crosslocations(\n",
    "  train_dhashes,\n",
    "  unique_train_locations_vs_test,\n",
    "  validation_dhashes,\n",
    "  unique_validation_locations\n",
    ")\n",
    "\n",
    "ntrain_sanitized = np.size(unique_train_locations_vs_validation)\n",
    "\n",
    "print(\n",
    "  'Number of unique images in train set and not in test set: %d'\n",
    "  % np.size(unique_train_locations_vs_test)\n",
    ")\n",
    "print(\n",
    "  'Number of unique images in train set and not in test or validation set: %d'\n",
    "  % ntrain_sanitized\n",
    ")\n",
    "print(\n",
    "  '%.2f%% of unique images in train dataset kept' \n",
    "  % round(\n",
    "    100*np.float32(ntrain_sanitized)/np.size(unique_train_locations_vs_test),\n",
    "    2\n",
    "  )\n",
    ")\n",
    "print(\n",
    "  '%.2f%% of images from original train dataset kept' \n",
    "  % round(100*np.float32(np.size(unique_train_locations_vs_validation))/ntrain, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images within validation dataset: 9281\n",
      "Number of unique images in validation set and not in test set: 8820\n",
      "95.03% of unique images in validation dataset kept\n",
      "88.20% of images from original validation dataset kept\n"
     ]
    }
   ],
   "source": [
    "## Get locations of images whose dHashes appear in validation but not in test set\n",
    "\n",
    "unique_validation_locations_vs_test = unique_image_crosslocations(\n",
    "  validation_dhashes,\n",
    "  unique_validation_locations,\n",
    "  test_dhashes,\n",
    "  unique_test_locations\n",
    ")\n",
    "\n",
    "nvalidation_sanitized = np.size(unique_validation_locations_vs_test)\n",
    "ntest_sanitized = np.size(unique_test_locations)\n",
    "\n",
    "print(\n",
    "  'Number of unique images within validation dataset: %d'\n",
    "  % np.size(unique_validation_locations)\n",
    ")\n",
    "print(\n",
    "  'Number of unique images in validation set and not in test set: %d'\n",
    "  % nvalidation_sanitized\n",
    ")\n",
    "print(\n",
    "  '%.2f%% of unique images in validation dataset kept' \n",
    "  % round(\n",
    "    100*np.float32(nvalidation_sanitized)/np.size(unique_validation_locations),\n",
    "    2\n",
    "  )\n",
    ")\n",
    "print(\n",
    "  '%.2f%% of images from original validation dataset kept' \n",
    "  % round(100*np.float32(nvalidation_sanitized)/nvalidation, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in original train set: 200000\n",
      "Number of images in sanitized train set: 151262\n",
      "75.63% of images from original train set kept in sanitized train set\n",
      "\n",
      "\n",
      "Number of images in original validation set: 10000\n",
      "Number of images in sanitized validation set: 8820\n",
      "88.20% of images from original validation set kept in sanitized validation set\n",
      "\n",
      "\n",
      "Number of images in original test set: 10000\n",
      "Number of images in sanitized test set: 8983\n",
      "89.83% of images from original test set kept in sanitized test set\n"
     ]
    }
   ],
   "source": [
    "## Summary of number of images in sanitized datasets\n",
    "\n",
    "print('Number of images in original train set: %d' % ntrain)\n",
    "print('Number of images in sanitized train set: %d' % ntrain_sanitized)\n",
    "print(\n",
    "  '%.2f%% of images from original train set kept in sanitized train set' \n",
    "  % round(100*np.float32(ntrain_sanitized)/ntrain, 2)\n",
    ")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Number of images in original validation set: %d' % nvalidation)\n",
    "print('Number of images in sanitized validation set: %d' % nvalidation_sanitized)\n",
    "print(\n",
    "  '%.2f%% of images from original validation set kept in sanitized validation set' \n",
    "  % round(100*np.float32(nvalidation_sanitized)/nvalidation, 2)\n",
    ")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Number of images in original test set: %d' % ntest)\n",
    "print('Number of images in sanitized test set: %d' % ntest_sanitized)\n",
    "print(\n",
    "  '%.2f%% of images from original test set kept in sanitized test set' \n",
    "  % round(100*np.float32(ntest_sanitized)/ntest, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding near-duplicates using Hamming distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dHashes of two images can be compared by calculating the Hamming distance of the dHashes, which is the count of differing characters between the two dHashes. The closer the Hamming distance to 0, the more similar the two images.\n",
    "\n",
    "In what follows, a function for computing the Hamming distance of two dHashes of equal length and a function for finding the number of unique images between two datasets with respect to a given Hamming distance lower bound are provided for demonstration purposes. In practice, the sanitized datasets have been generated by keeping images with unique dHashes, which corresponds to the case of images whose pairwise dHashes have Hamming distance equal to the hash length (16). This choice was made for the sake of computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Return the Hamming distance between equal-length hashes\n",
    "\n",
    "def hamming_distance(h1, h2):\n",
    "  if len(h1) != len(h2):\n",
    "    raise ValueError(\"Undefined for hashes of unequal length\")\n",
    "\n",
    "  return sum(c1 != c2 for c1, c2 in zip(h1, h2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function for returning locations (array indices) of unique images between datasets\n",
    "## Uniqueness of images is defined on the basis of a given Hamming distance lower bound\n",
    "## The returned indices are a subset of scannedlocs\n",
    "## scannedlocs is a subset of baselocs\n",
    "\n",
    "def unique_image_hamming_crosslocations(\n",
    "  scannedset, scannedlocs, baseset, baselocs, lb, monitor=None):\n",
    "  indices = np.array([], dtype=np.int64)\n",
    "\n",
    "  if monitor is not None:\n",
    "    k = 0\n",
    "    n = np.size(scannedlocs)\n",
    "    \n",
    "  for i in scannedlocs:\n",
    "    if monitor is not None:\n",
    "      k += 1\n",
    "      if k % monitor == 0:\n",
    "        print('%.2f%% completed' % round(100*np.float32(k)/n, 2))\n",
    "\n",
    "    u = True\n",
    "    \n",
    "    for j in baselocs:\n",
    "      if hamming_distance(scannedset[i], baseset[j]) < lb:\n",
    "        u = False\n",
    "        break\n",
    "\n",
    "    if u:\n",
    "      indices = np.append(indices, i)\n",
    "    \n",
    "  return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unique_image_hamming_crosslocations(\n",
    "#   validation_dhashes,\n",
    "#   unique_validation_locations,\n",
    "#   test_dhashes,\n",
    "#   unique_test_locations,\n",
    "#   1,\n",
    "#   1000\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store sanitized train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/theodore/workspace/self_education/udacity/TensorFlowUdacity/data/notmnist/sanitized/notmnist.pickle already present, skipping pickling\n"
     ]
    }
   ],
   "source": [
    "## Save sanitized train, validation and test datasets to pickle file\n",
    "\n",
    "save_to_pickle(\n",
    "  SANITIZEDDATAFILE,\n",
    "  {\n",
    "    'train_dataset': train_dataset[unique_train_locations_vs_validation, :, :],\n",
    "    'train_labels': train_labels[unique_train_locations_vs_validation],\n",
    "    'ntrain' : np.size(unique_train_locations_vs_validation),\n",
    "    'validation_dataset': validation_dataset[unique_validation_locations_vs_test, :, :],\n",
    "    'validation_labels': validation_labels[unique_validation_locations_vs_test],\n",
    "    'nvalidation': np.size(unique_validation_locations_vs_test),\n",
    "    'test_dataset': test_dataset[unique_test_locations, :, :],\n",
    "    'test_labels': test_labels[unique_test_locations],\n",
    "    'ntest' : np.size(unique_test_locations)\n",
    "  }    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 530446976\n"
     ]
    }
   ],
   "source": [
    "## Size of picle file containing sanitized train, validation and test datasets\n",
    "\n",
    "print('Compressed pickle size: %d' % os.stat(SANITIZEDDATAFILE).st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be completed soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load pickled sanitized train, validation and test datasets\n",
    "\n",
    "with open(SANITIZEDDATAFILE, 'rb') as f:\n",
    "  sane_datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Construct test set to be used for logistic regression\n",
    "## Retain all images in the sanitized test set\n",
    "\n",
    "test_indices = np.arange(sane_datasets['ntest'])\n",
    "\n",
    "n_test = np.size(test_indices)\n",
    "\n",
    "test_X = sane_datasets['test_dataset'][test_indices, :, :]\n",
    "test_X = test_X.reshape(n_test, np.shape(test_X)[1]*np.shape(test_X)[2])\n",
    "\n",
    "test_y = sane_datasets['test_labels'][test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Construct train set to be used for logistic regression\n",
    "## Randomly pick 50 images from the sanitized train set\n",
    "\n",
    "n_train = 50\n",
    "\n",
    "train_indices = \\\n",
    "  np.random.choice(np.arange(sane_datasets['ntrain']), n_train, replace=False)\n",
    "\n",
    "train_X = sane_datasets['train_dataset'][train_indices, :, :]\n",
    "train_X = train_X.reshape(n_train, np.shape(train_X)[1]*np.shape(train_X)[2])\n",
    "\n",
    "train_y = sane_datasets['train_labels'][train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define logistic regression model\n",
    "\n",
    "logreg = LogisticRegression(C=1e5, multi_class='multinomial', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "          random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "          warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fit logistic regression model to the 50 training points\n",
    "\n",
    "logreg.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Get the configuration of the logistic regression model\n",
    "\n",
    "logreg.get_params();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Get estimated coefficients\n",
    "\n",
    "# logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64906417112299464"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get mean accuracy of built model on the test set\n",
    "\n",
    "logreg.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in test set: 8976\n",
      "Number of correctly predicted images in test set: 5826\n",
      "0.65% of images in test set correctly predicted\n"
     ]
    }
   ],
   "source": [
    "print('Number of images in test set: %d' % n_test)\n",
    "print(\n",
    "  'Number of correctly predicted images in test set: %d'\n",
    "  % sum(np.equal(logreg.predict(test_X), test_y)))\n",
    "print(\n",
    "  '%.2f%% of images in test set correctly predicted' \n",
    "  % round(np.float32(sum(np.equal(logreg.predict(test_X), test_y)))/n_test, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65274064171122992"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fit logistic regression on a train set consisting of 100 samples\n",
    "\n",
    "n_train = 100\n",
    "\n",
    "train_indices = \\\n",
    "  np.random.choice(np.arange(sane_datasets['ntrain']), n_train, replace=False)\n",
    "\n",
    "train_X = sane_datasets['train_dataset'][train_indices, :, :]\n",
    "train_X = train_X.reshape(n_train, np.shape(train_X)[1]*np.shape(train_X)[2])\n",
    "\n",
    "train_y = sane_datasets['train_labels'][train_indices]\n",
    "\n",
    "logreg = LogisticRegression(C=1e5, multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "logreg.fit(train_X, train_y)\n",
    "\n",
    "logreg.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78631907308377902"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fit logistic regression on a train set consisting of 1000 samples\n",
    "\n",
    "n_train = 1000\n",
    "\n",
    "train_indices = \\\n",
    "  np.random.choice(np.arange(sane_datasets['ntrain']), n_train, replace=False)\n",
    "\n",
    "train_X = sane_datasets['train_dataset'][train_indices, :, :]\n",
    "train_X = train_X.reshape(n_train, np.shape(train_X)[1]*np.shape(train_X)[2])\n",
    "\n",
    "train_y = sane_datasets['train_labels'][train_indices]\n",
    "\n",
    "logreg = LogisticRegression(C=1e5, multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "logreg.fit(train_X, train_y)\n",
    "\n",
    "logreg.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7679367201426025"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fit logistic regression on a train set consisting of 5000 samples\n",
    "\n",
    "n_train = 5000\n",
    "\n",
    "train_indices = \\\n",
    "  np.random.choice(np.arange(sane_datasets['ntrain']), n_train, replace=False)\n",
    "\n",
    "train_X = sane_datasets['train_dataset'][train_indices, :, :]\n",
    "train_X = train_X.reshape(n_train, np.shape(train_X)[1]*np.shape(train_X)[2])\n",
    "\n",
    "train_y = sane_datasets['train_labels'][train_indices]\n",
    "\n",
    "logreg = LogisticRegression(C=1e5, multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "logreg.fit(train_X, train_y)\n",
    "\n",
    "logreg.score(test_X, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
