{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define environment variables\n",
    "\n",
    "BASEDIR = \"/Users/theodore/workspace/pycharm/TensorFlowUdacity\"\n",
    "\n",
    "DATADIR = os.path.join(BASEDIR, \"data\")\n",
    "TRAINDATADIR = os.path.join(DATADIR, \"notmnist\", \"notmnist_large\")\n",
    "TESTDATADIR = os.path.join(DATADIR, \"notmnist\", \"notmnist_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Functions for getting array of directory paths and array of file paths\n",
    "\n",
    "def get_dir_paths(root):\n",
    "  return [os.path.join(root, n) for n in sorted(os.listdir(root)) if os.path.isdir(os.path.join(root, n))]\n",
    "\n",
    "def get_file_paths(root):\n",
    "  return [os.path.join(root, n) for n in sorted(os.listdir(root)) if os.path.isfile(os.path.join(root, n))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Get directory and file paths of training and test sets\n",
    "\n",
    "train_data_paths = get_dir_paths(TRAINDATADIR)\n",
    "test_data_paths = get_dir_paths(TESTDATADIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Display a sample of 5 images in their initial png format\n",
    "\n",
    "nsamples = 5\n",
    "\n",
    "for i in np.arange(nsamples):\n",
    "    display(Image(filename=np.random.choice(get_file_paths(np.random.choice(test_data_paths)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set image properties\n",
    "\n",
    "image_size = 28 # Pixel width and height\n",
    "pixel_depth = 255.0  # Number of levels per pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read a sample image\n",
    "\n",
    "image_file = np.random.choice(get_file_paths(np.random.choice(test_data_paths)))\n",
    "image_data = ndimage.imread(image_file).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Show numeric representation of image\n",
    "\n",
    "image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Show type of image object\n",
    "\n",
    "type(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Show dimensions of image object\n",
    "\n",
    "image_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot image using imshow\n",
    "\n",
    "plt.imshow(image_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot image using a scatterplot\n",
    "\n",
    "colors = [str(i/pixel_depth) for i in np.ravel(image_data)]\n",
    "plt.scatter(\n",
    "    np.tile(np.arange(image_size), image_size),\n",
    "    np.repeat(np.flipud(np.arange(image_size)), image_size),\n",
    "    s=150,\n",
    "    c=colors,\n",
    "    marker='s'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Plot image using a scatterplot by setting cmap option\n",
    "\n",
    "colors = [str(i/pixel_depth) for i in np.ravel(image_data)]\n",
    "plt.scatter(\n",
    "    np.tile(np.arange(image_size), image_size),\n",
    "    np.repeat(np.flipud(np.arange(image_size)), image_size),\n",
    "    s=150,\n",
    "    c=colors,\n",
    "    marker='s',\n",
    "    cmap=plt.cm.viridis    \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function for loading data for a single letter\n",
    "\n",
    "def load_letter(root, image_size, pixel_depth, verbose=True, min_nimages=1):\n",
    "  \"\"\"Load data for a single letter.\"\"\"\n",
    "\n",
    "  if verbose:\n",
    "        print(root)\n",
    "\n",
    "  image_files = get_file_paths(root)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32)\n",
    "\n",
    "  image_index = 0\n",
    "  for image in image_files:\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image).astype(float)-pixel_depth/2)/pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[image_index, :, :] = image_data\n",
    "      image_index += 1\n",
    "    except IOError as e:\n",
    "      print('Skipping because of not being able to read: ', image_file)\n",
    "\n",
    "  dataset = dataset[0:image_index, :, :]\n",
    "  if image_index < min_nimages:\n",
    "    raise Exception('Fewer images than expected: %d < %d' % (image_index, min_nimages))\n",
    "\n",
    "  if verbose:    \n",
    "    print('Full dataset tensor: ', dataset.shape)\n",
    "    print('Mean: ', np.mean(dataset))\n",
    "    print('Standard deviation: ', np.std(dataset))\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letter_data = load_letter(test_data_paths[2], image_size, pixel_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letter_data[0, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letter_data[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function for pickling data of all letters\n",
    "\n",
    "def pickle_letters(root, image_size, pixel_depth, verbose=True, min_nimages=1, force=False):\n",
    "  dataset_files = []\n",
    "  for d in root:\n",
    "    pickle_file = d + '.pickle'\n",
    "    dataset_files.append(pickle_file)\n",
    "    if os.path.exists(pickle_file) and not force:\n",
    "      print('%s already present, skipping pickling' % pickle_file)\n",
    "    else:\n",
    "      print('Pickling %s' % pickle_file)\n",
    "      dataset = load_letter(d, image_size, pixel_depth, verbose=verbose, min_nimages=min_nimages)\n",
    "      try:\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "  \n",
    "  return dataset_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_datasets = pickle_letters(train_data_paths, image_size, pixel_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_datasets = pickle_letters(test_data_paths, image_size, pixel_depth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
